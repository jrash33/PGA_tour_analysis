{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGA Tour Player Performance: Web Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL of page to be scraped\n",
    "url = 'https://www.pgatour.com/players.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve page with the requests module\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create BeautifulSoup object; parse w 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scrape Link to Each Player Stat Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #execute chromedriver\n",
    "# executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "# browser = Browser('chrome', **executable_path, headless=False)\n",
    "# browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html code via beautifulsoup\n",
    "# html = browser.html\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "#soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get names of all the player links\n",
    "#retrieve the parent divs for all links\n",
    "players = soup.find_all('span',class_=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty list to accept data\n",
    "player_names = []\n",
    "player_url = []\n",
    "\n",
    "#loop through each parent div and grab the link to the player stat page\n",
    "for player in players:\n",
    "    #get name of player\n",
    "    player_names.append(player.a.text)\n",
    "    #get url for player performance page\n",
    "    player_url.append(player.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape PGA Performance Data for Each Individual Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.pgatour.com/players/player.08793.tiger-woods.html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create url for player\n",
    "base_url = 'https://www.pgatour.com'\n",
    "test_url = player_url[802]\n",
    "scrape_url = base_url + test_url\n",
    "#scrape_url = \"https://www.pgatour.com/players/player.01006.john-adams.html\"\n",
    "\n",
    "#go to url page\n",
    "response = requests.get(scrape_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "scrape_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splinter Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute chromedriver\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(scrape_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "browser.click_link_by_partial_text(\"Performance\")\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test to see if javascript rendered data was scraped properly\n",
    "# test = soup.find('div', class_='wrap').find('div', class_='tabbable').find('div', class_=\"performance\").find('div',class_='tab-content') \\\n",
    "# .find('div', class_='tab-pane')\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#photo of player\n",
    "try:\n",
    "    photo_url = soup.find('img', class_='photo')['src']\n",
    "    #player_intro = [{'Player Name':}]\n",
    "except TypeError:\n",
    "    photo_url = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scrape all html hstat code\n",
    "info = soup.findAll('div', class_='item')\n",
    "\n",
    "#headline stats\n",
    "info_stats = []\n",
    "\n",
    "for i in info:\n",
    "    try:\n",
    "        #get stat name\n",
    "        caption = i.find('div',class_='denotation').text\n",
    "        #get stat value\n",
    "        value = i.find('div', class_='value').text\n",
    "        #clean up text\n",
    "        value = value.replace('\\xa0','')\n",
    "        value = value.replace('\\n','')\n",
    "        #save to dictionary\n",
    "        #post = {'caption':caption, 'value': value}\n",
    "        post = {caption: value}\n",
    "        #append to list\n",
    "        info_stats.append(post)\n",
    "    except AttributeError:\n",
    "        nothing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scrape all html hstat code\n",
    "hstats = soup.findAll('div', class_='stat')\n",
    "\n",
    "#headline stats\n",
    "h_stats = []\n",
    "for hstat in hstats:\n",
    "    try:\n",
    "        #get stat name\n",
    "        caption = hstat.find('div',class_='caption').text\n",
    "        #get stat value\n",
    "        value = hstat.find('div',class_='value').text\n",
    "        \n",
    "        #save to dictionary\n",
    "        #post = {'caption':caption, 'value': value}\n",
    "        post = {caption : value}\n",
    "\n",
    "        #append to list\n",
    "        h_stats.append(post)\n",
    "    except AttributeError:\n",
    "        nothing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scrape all html astat code\n",
    "astats = soup.findAll('tr')\n",
    "\n",
    "#attribute stats\n",
    "a_stats = []\n",
    "\n",
    "for astat in astats:\n",
    "    try:\n",
    "        #get the stat name\n",
    "        caption = astat.find('td',class_='caption').text\n",
    "        #get stat value\n",
    "        value = astat.find('td',class_='value').text\n",
    "        \n",
    "        #save to dictionary\n",
    "        post = {caption : value}\n",
    "        \n",
    "        #append to list\n",
    "        a_stats.append(post)\n",
    "    except AttributeError:\n",
    "        nothing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scrape for additional needed info\n",
    "extrastats = soup.findAll('td')\n",
    "\n",
    "#attribute stats\n",
    "extra_stats = []\n",
    "\n",
    "for extra in extrastats:\n",
    "    try:\n",
    "        #get the stat name\n",
    "        text = extra.text\n",
    "        #append to list\n",
    "        extra_stats.append(text)\n",
    "    except AttributeError:\n",
    "        nothing=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these attributes are unique with no captions/values-All string format\n",
    "#search sub_strings of desired variables for values\n",
    "sub_strings = ['Total Left rough', 'Total Right rough', 'Possible Fwys', 'Distance Rank', 'Accuracy Rank',\n",
    "      'Total Club Head Speed', 'Total Attempts']\n",
    "\n",
    "extra_stats_var= []\n",
    "for sub in sub_strings:\n",
    "    x = [s for s in extra_stats if sub in s]\n",
    "    if x:\n",
    "        x = x[0].split(':')\n",
    "        x[1] = x[1].replace(' ','')\n",
    "\n",
    "        #post = {'caption': x[0], 'value': x[1]}\n",
    "        post = {x[0] : x[1]}\n",
    "        extra_stats_var.append(post)\n",
    "    else:\n",
    "        nothing=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function that takes desired variables with your list of dictionaries scraped and returns a clean list of these variables\n",
    "def get_vars(stats, vars_wanted):\n",
    "    stats_vars = []\n",
    "    items=[]\n",
    "    #iterate through scraped data to find desired variables\n",
    "    for list_item in stats:\n",
    "        dict_item = [key for key,value in list_item.items()]\n",
    "        if dict_item[0] in vars_wanted:\n",
    "            #check for duplicates\n",
    "            if dict_item not in items:\n",
    "                items.append(dict_item)\n",
    "                stats_vars.append(list_item)\n",
    "    return stats_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables wanted from info stats\n",
    "info_var = ['Height', 'Weight', 'AGE', 'Turned Pro', 'College', 'Birthplace' ,'FEDEXCUP Rank', 'FEDEXCUP Points', 'Scoring Average']\n",
    "#variables wanted from headline stats\n",
    "h_var = ['Total Distance', 'Total Drives', '# of Drives', 'Fairways Hit', 'Possible Fairways', 'Measured Rounds']\n",
    "#variables wanted from additional stats\n",
    "a_var = ['Driving Distance','Driving Accuracy Percentage','Total Driving','Club Head Speed',\n",
    "         'Distance from Edge of Fairway','Left Rough Tendency','Right Rough Tendency','Total Driving Efficiency']\n",
    "extra_var = ['Total Left rough', 'Total Right rough', 'Possible Fwys', 'Distance Rank', 'Accuracy Rank',\n",
    "      'Total Club Head Speed', 'Total Attempts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_stats_vars = get_vars(info_stats, info_var)\n",
    "h_stats_vars = get_vars(h_stats, h_var)\n",
    "a_stats_vars = get_vars(a_stats, a_var)\n",
    "extra_stats_vars = get_vars(extra_stats_var, sub_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine all stat variables\n",
    "# old method : all_stat_var = info_stats_vars + h_stats_vars + a_stats_vars + extra_stats_vars\n",
    "all_stat_var = {}\n",
    "\n",
    "a_stats_vars = {key: value for a_stat in a_stats_vars for key, value in a_stat.items()}\n",
    "all_stat_var.update(a_stats_vars)\n",
    "\n",
    "h_stats_vars = {key: value for h_stat in h_stats_vars for key, value in h_stat.items()}\n",
    "all_stat_var.update(h_stats_vars)\n",
    "\n",
    "extra_stats_vars = {key: value for extra_stat in extra_stats_vars for key, value in extra_stat.items()}\n",
    "all_stat_var.update(extra_stats_vars)\n",
    "\n",
    "info_stats_vars = {key: value for info_stat in info_stats_vars for key, value in info_stat.items()}\n",
    "all_stat_var.update(info_stats_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "bla = soup.findAll('div', class_ = 'holder')\n",
    "dates_ = soup.findAll('td', class_ = 'date')\n",
    "rounds = soup.findAll('td', class_='round')\n",
    "\n",
    "tourney_name = []\n",
    "all_text = []\n",
    "scores = []\n",
    "to_par = []\n",
    "pos = []\n",
    "dates = []\n",
    "\n",
    "for i in bla:\n",
    "    x = i.find('tbody')\n",
    "    #tourney info\n",
    "    tourneys = x.findAll('p')\n",
    "    #need this for all text\n",
    "    tds = x.findAll('td')\n",
    "    #get all text to use later for pos\n",
    "    [all_text.append(td.text) for td in tds]\n",
    "    \n",
    "    #tournament names\n",
    "    [tourney_name.append(j.text) for j in tourneys]\n",
    "\n",
    "#clean dates\n",
    "[dates.append(d.text) for d in dates_]\n",
    "#scores of each round in increments of 4 ('--' means no score)\n",
    "[scores.append(r.text) for r in rounds]\n",
    "#now append tournament position results by getting list item after tournament name\n",
    "[pos.append(all_text[all_text.index(tourney)+1]) for tourney in tourney_name]\n",
    "#now append tournament position results by getting list item after tournament name\n",
    "[to_par.append(all_text[all_text.index(tourney)+8]) for tourney in tourney_name]\n",
    "#delete first one\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final dictionary of tournaments for the past year\n",
    "tournament_history = []\n",
    "for date,tourney,score,rank in zip(dates,tourney_name,to_par,pos):\n",
    "    try:\n",
    "        #create dictionary with all info\n",
    "        post = {'Date':date, \n",
    "                'Tournament Name':tourney, \n",
    "                'Total Score':score, \n",
    "                'POS':rank}\n",
    "        #append to final list\n",
    "        tournament_history.append(post)\n",
    "    except AttributeError:\n",
    "        nothing=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#output list\n",
    "player_final = {}\n",
    "player_final['player_intro'] = photo_url\n",
    "player_final['all_stat_var'] = all_stat_var\n",
    "player_final['tournament_hist'] = tournament_history\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
